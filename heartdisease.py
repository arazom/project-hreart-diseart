# -*- coding: utf-8 -*-
"""HeartDisease.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11rsDQpLnC5YiqNk5VscPIRMiVNNBT90o
"""

! pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip

import pandas as pd
pd.__version__

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from pandas_profiling import ProfileReport
import plotly.express as px
import plotly.offline as py
from sklearn.metrics import accuracy_score, classification_report, plot_roc_curve, plot_confusion_matrix, f1_score, ConfusionMatrixDisplay

heart = pd.read_csv("heart.csv")

heart.head()

heart.shape

"""##DataSet INFO

1. Age : displays the age of the individual.
2. Sex : displays the gender of the individual using the following format :  

*   1 = male
*   0 = female.

 3.(ChestPainType) Chest-pain type : displays the type of chest-pain experienced by the individual using the following format:

  * 1 = typical angina

  * 2 = atypical angina

  * 3 = non - anginal pain

  * 4 = asymptotic

4. (RestingBP)Resting Blood Pressure : displays the resting blood pressure value of an individual in mmHg (unit)

5. (Cholesterol)Serum Cholestrol : displays the serum cholestrol in mg/dl (unit)

6. (FastingBS)Fasting Blood Sugar : compares the fasting blood sugar value of an individual with 120mg/dl.
 If fasting blood sugar > 120mg/dl then :
* 1 (true) else

* 0 (false)

7. (RestingECG)Resting ECG :

* 0 = normal
* 1 = having ST-T wave abnormality
* 2 = left ventricular hyperthrophy

 The resting electrocardiogram is a test that measures the electrical activity of the heart

8. (MaxHR)Max heart rate achieved : displays the max heart rate achieved by an individual.

9. (ExerciseAngina)Exercise induced angina :

1 = yes

0 = no

10.(Oldpeak) ST depression induced by exercise relative to rest : displays the value which is integer or float.

11. (ST_Slope)Peak exercise ST segment :

 1 = upsloping

 2 = flat

 3 = downsloping

12 . (HeartDisease)Diagnosis of heart disease : Displays whether the individual is suffering from heart disease or not :
 0 = absence
1 = present.

### Fast Report
"""

profile=ProfileReport(heart, title=" Heart Disease ", html={'style': {'full_width': True}})

profile.to_notebook_iframe()
profile.to_file(output_file='Heart_Disease.html')

"""## Discover and visualize the data"""

heart.hist(figsize=(15,15))

corr_matrix=heart.corr()
corr_matrix['HeartDisease'].sort_values(ascending=False)

plt.figure(figsize=(15,5))
sns.heatmap(heart.corr(),cmap='RdYlBu',annot=True)
plt.show()

"""# Target Distribution"""

px.pie(heart, names='HeartDisease', color_discrete_sequence=px.colors.sequential.Aggrnyl)

"""we have imbalance Data
more than half of samples are fine (1=m , 0= f

# Age Distribution
"""

px.histogram(heart,
             x='Age',
             color_discrete_sequence=px.colors.sequential.Aggrnyl,
             hover_data=heart.columns, 
             marginal="box",
             color='HeartDisease')

"""The above Distribution figure depicts the distribution of Age across all of the heart patient entries in the dataset. According to the graph, the age group 54-55 years has the largest number of patients suffering from heart disease. Patients between the ages of 29 and 31 are far less likely to develop heart disease.

# Gender Distribution
"""

df=heart.copy()
df['Sex']=df['Sex'].map({
    1:'M',
    0:'F'
})

px.pie(df, names='Sex',
       color_discrete_sequence=px.colors.sequential.Aggrnyl,
       hole=.6,
       
       )

"""The above Pie chart depicts the gender distribution of heart disease patients. Looking at the graph, we may deduce that males are two times more likely than females to suffer from heart disease.

# Relation of ECG measurement with Target
"""

px.histogram(heart, x="HeartDisease", color="RestingECG",barmode="group",color_discrete_sequence=px.colors.sequential.Aggrnyl)

"""The above graph demonstrates that  restscg value 0 , 1,2 are not so afect to develop heart disease.

agreater number of patients with restscg value 0 are less likely to get heart disease, but a greater number of persons with restscg value 1 are more likely to develop heart disease.

# Relation of Cholestrol with Target
"""

px.violin(heart, y="Cholesterol",
           color="HeartDisease",
           color_discrete_sequence=px.colors.sequential.Aggrnyl,
           hover_data=heart.columns, 
           points="all",
           box=True ,
           )

"""The above  plot of cholestrol levels vs. target reveals that people who are more likely to develop heart disease have greater cholestrol levels than those who have goal 0 (not likely to develop heart disease).

## **Cleaning** the Data

# Text and Categorical Attributes
"""

heart.head()

print(heart["Sex"].unique())
print(heart["ChestPainType"].unique())
print(heart["RestingECG"].unique())
print(heart["ExerciseAngina"].unique())
print(heart["ST_Slope"].unique())

heart['HeartDisease'].unique()

# using replace function
heart.replace({"M": 1, "F": 0}, inplace=True)
heart.replace({'ATA': 1, 'NAP': 2, 'ASY': 3, 'TA': 4}, inplace=True)
heart.replace({'Normal': 0, 'ST': 1, 'LVH': 2}, inplace=True)
heart.replace({'N': 0, 'Y': 2}, inplace=True)
heart.replace({'Up': 1, 'Flat': 2, 'Down': 3}, inplace=True)

"""## Feature Extraction"""

heart.head()

"""# Splitting the data"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report,accuracy_score

Y = heart['HeartDisease']
X = heart.drop('HeartDisease',axis=1)
# x, x_test, y, y_test = train_test_split(X,Y,test_size=0.2,train_size=0.8)
x_train, x_validation, y_train, y_validation = train_test_split(X,Y,test_size=0.1,train_size=0.9)

"""# **Evalution Fuction**"""

# Evaluation
from sklearn.metrics import confusion_matrix, f1_score
def evaluation(model):
  y_pred_model = model.predict(x_validation)
  print(f"Accuracy : {model.score(x_validation,y_validation)}")
  print(f"CM : \n {confusion_matrix(y_validation,y_pred_model)}")
  print(f"F1 Score  : {f1_score(y_validation,y_pred_model)}")

"""# Models

# Random Forest Classifier
"""

from sklearn.ensemble import RandomForestClassifier
forest_reg = RandomForestClassifier(max_depth=20)
forest_reg.fit(x_train, y_train)

evaluation(forest_reg)

import numpy as np
from sklearn.model_selection import cross_val_score
scores = cross_val_score(forest_reg, x_train, y_train,
scoring="neg_mean_squared_error", cv=10)
rmse_scores = np.sqrt(-scores)
print("Mean (with Cross_val)", rmse_scores.mean())

"""# accurcy on Train set"""

# since the most promising algorithm is Random forest 
# we will find the accurecy on the test set
import numpy as np
from sklearn.metrics import mean_squared_error
predictCLF = forest_reg.predict(x_validation)
forest_mse = mean_squared_error(y_validation, predictCLF)
forest_rmse = np.sqrt(forest_mse)
print("RMSE",forest_rmse)
test_accuracy = forest_reg.score(x_validation, y_validation)*100
print("Accuracy:", test_accuracy)

print(classification_report(predictCLF,y_validation))

"""# Vector machine"""

from sklearn.svm import SVC
vector_machine = SVC()
vector_machine.fit(x_train,y_train)

evaluation(vector_machine)

import numpy as np
from sklearn.model_selection import cross_val_score
scores = cross_val_score(vector_machine, x_train, y_train,
scoring="neg_mean_squared_error", cv=10)
rmse_scores = np.sqrt(-scores)
print("Mean (with Cross_val)", rmse_scores.mean())

"""# Let's Try tuning it"""

# Tuning
from sklearn.model_selection import GridSearchCV
gs_svc = GridSearchCV(vector_machine,param_grid={"C":[0.001,0.01,0.1,1.0,10.0,100.0,1000.0]},cv=10)
gs_svc.fit(x_train,y_train)

gs_svc.best_params_

best_svc = gs_svc.best_estimator_
best_svc.fit(x_train,y_train)
evaluation(best_svc)

predictCLF1 = vector_machine.predict(x_validation)

print(classification_report(predictCLF1,y_validation))

import numpy as np
from sklearn.model_selection import cross_val_score
scores = cross_val_score(best_svc, x_train, y_train,
scoring="neg_mean_squared_error", cv=10)
rmse_scores = np.sqrt(-scores)
print("Mean (with Cross_val)", rmse_scores.mean())

# since the most promising algorithm is Random forest 
# we will find the accurecy on the test set
import numpy as np
from sklearn.metrics import mean_squared_error

predictCLF = vector_machine.predict(x_validation)
forest_mse = mean_squared_error(y_validation, predictCLF)
forest_rmse = np.sqrt(forest_mse)
print("RMSE",forest_rmse)
test_accuracy = vector_machine.score(x_validation, y_validation)*100
print("Accuracy:", test_accuracy)

"""# KNeighbors classification"""

from sklearn.neighbors import KNeighborsClassifier
np.random.seed(42)
clff = KNeighborsClassifier()
clff.fit(x_train, y_train);

evaluation(clff)

# since the most promising algorithm is Random forest 
# we will find the accurecy on the test set
import numpy as np
from sklearn.metrics import mean_squared_error

predictCLF = clff.predict(x_validation)
f_mse = mean_squared_error(y_validation, predictCLF)
f_rmse = np.sqrt(f_mse)
print("RMSE",f_rmse)
test_accuracy = clff.score(x_validation, y_validation)*100
print("Accuracy:", test_accuracy)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
def getScore(model, x_validation, y_validation):
    y_pred = model.predict(x_validation)
    print('f1_score')
    print(f1_score(y_validation,y_pred,average='binary'))
    print('accuracy')
    acc = accuracy_score(y_validation,y_pred, normalize=True)
    print(acc)
    print('Confusion Matrix :')
    
    confusion_matrix(y_validation, y_pred, labels=model.classes_)
    ConfusionMatrixDisplay(confusion_matrix, display_labels=model.classes_)
    plt.show()
    return acc

clf_accuracy = getScore(vector_machine, x_validation, y_validation)

evaluation(forest_reg)

print(classification_report(predictCLF,y_validation))

acc_CLF=accuracy_score(predictCLF,y_validation)
print(acc_CLF)